{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import mne\n",
    "import os, sys\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"./code\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder = 'C:/Users/Micro/Desktop/MainResearchProjects/SNN-seizure-detection-data/ethz_ieeg/long-term/ID01/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG_raw_files = os.listdir(raw_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eeg_recording import SingleSubjectRecording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "example_data = scipy.io.loadmat(os.path.join(raw_data_folder, EEG_raw_files[0]))['EEG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=88, n_times=1843200\n",
      "    Range : 0 ... 1843199 =      0.000 ...  3599.998 secs\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "mne_example_data = mne.io.RawArray(example_data, mne.create_info(ch_names=[f'eeg unknow_ch_{i}'for i in range(example_data.shape[0])], sfreq=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micro\\AppData\\Local\\Temp\\ipykernel_20880\\1345481350.py:1: RuntimeWarning: The unit for channel(s) eeg unknow_ch_0, eeg unknow_ch_1, eeg unknow_ch_10, eeg unknow_ch_11, eeg unknow_ch_12, eeg unknow_ch_13, eeg unknow_ch_14, eeg unknow_ch_15, eeg unknow_ch_16, eeg unknow_ch_17, eeg unknow_ch_18, eeg unknow_ch_19, eeg unknow_ch_2, eeg unknow_ch_20, eeg unknow_ch_21, eeg unknow_ch_22, eeg unknow_ch_23, eeg unknow_ch_24, eeg unknow_ch_25, eeg unknow_ch_26, eeg unknow_ch_27, eeg unknow_ch_28, eeg unknow_ch_29, eeg unknow_ch_3, eeg unknow_ch_30, eeg unknow_ch_31, eeg unknow_ch_32, eeg unknow_ch_33, eeg unknow_ch_34, eeg unknow_ch_35, eeg unknow_ch_36, eeg unknow_ch_37, eeg unknow_ch_38, eeg unknow_ch_39, eeg unknow_ch_4, eeg unknow_ch_40, eeg unknow_ch_41, eeg unknow_ch_42, eeg unknow_ch_43, eeg unknow_ch_44, eeg unknow_ch_45, eeg unknow_ch_46, eeg unknow_ch_47, eeg unknow_ch_48, eeg unknow_ch_49, eeg unknow_ch_5, eeg unknow_ch_50, eeg unknow_ch_51, eeg unknow_ch_52, eeg unknow_ch_53, eeg unknow_ch_54, eeg unknow_ch_55, eeg unknow_ch_56, eeg unknow_ch_57, eeg unknow_ch_58, eeg unknow_ch_59, eeg unknow_ch_6, eeg unknow_ch_60, eeg unknow_ch_61, eeg unknow_ch_62, eeg unknow_ch_63, eeg unknow_ch_64, eeg unknow_ch_65, eeg unknow_ch_66, eeg unknow_ch_67, eeg unknow_ch_68, eeg unknow_ch_69, eeg unknow_ch_7, eeg unknow_ch_70, eeg unknow_ch_71, eeg unknow_ch_72, eeg unknow_ch_73, eeg unknow_ch_74, eeg unknow_ch_75, eeg unknow_ch_76, eeg unknow_ch_77, eeg unknow_ch_78, eeg unknow_ch_79, eeg unknow_ch_8, eeg unknow_ch_80, eeg unknow_ch_81, eeg unknow_ch_82, eeg unknow_ch_83, eeg unknow_ch_84, eeg unknow_ch_85, eeg unknow_ch_86, eeg unknow_ch_87, eeg unknow_ch_9 has changed from NA to V.\n",
      "  mne_example_data.set_channel_types(dict(zip(mne_example_data.ch_names, ['eeg'] * len(mne_example_data.ch_names))))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<details open>\n",
       "    <summary><strong>General</strong></summary>\n",
       "    <table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "        <tr>\n",
       "            <th>Measurement date</th>\n",
       "            \n",
       "            <td>Unknown</td>\n",
       "            \n",
       "        </tr>\n",
       "        <tr>\n",
       "            <th>Experimenter</th>\n",
       "            \n",
       "            <td>Unknown</td>\n",
       "            \n",
       "        </tr>\n",
       "        <tr>\n",
       "            <th>Participant</th>\n",
       "            \n",
       "            <td>Unknown</td>\n",
       "            \n",
       "        </tr>\n",
       "    </table>\n",
       "    </details>\n",
       "    <details open>\n",
       "        <summary><strong>Channels</strong></summary>\n",
       "        <table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "            <tr>\n",
       "                <th>Digitized points</th>\n",
       "                \n",
       "                <td>Not available</td>\n",
       "                \n",
       "            </tr>\n",
       "            <tr>\n",
       "                <th>Good channels</th>\n",
       "                <td>88 EEG</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <th>Bad channels</th>\n",
       "                <td>None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <th>EOG channels</th>\n",
       "                <td>Not available</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <th>ECG channels</th>\n",
       "                <td>Not available</td>\n",
       "            </tr>\n",
       "        </table>\n",
       "        </details>\n",
       "        <details open>\n",
       "            <summary><strong>Data</strong></summary>\n",
       "            <table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "                \n",
       "                <tr>\n",
       "                    <th>Sampling frequency</th>\n",
       "                    <td>512.00 Hz</td>\n",
       "                </tr>\n",
       "                \n",
       "                \n",
       "                <tr>\n",
       "                    <th>Highpass</th>\n",
       "                    <td>0.00 Hz</td>\n",
       "                </tr>\n",
       "                \n",
       "                \n",
       "                <tr>\n",
       "                    <th>Lowpass</th>\n",
       "                    <td>256.00 Hz</td>\n",
       "                </tr>\n",
       "                \n",
       "                \n",
       "                \n",
       "                \n",
       "                <tr>\n",
       "                    <th>Duration</th>\n",
       "                    <td>00:59:60 (HH:MM:SS)</td>\n",
       "                </tr>\n",
       "                \n",
       "            </table>\n",
       "            </details>"
      ],
      "text/plain": [
       "<RawArray | 88 x 1843200 (3600.0 s), ~1.21 GB, data loaded>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mne_example_data.set_channel_types(dict(zip(mne_example_data.ch_names, ['eeg'] * len(mne_example_data.ch_names))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG channel type selected for re-referencing\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n"
     ]
    }
   ],
   "source": [
    "mne_example_data = mne_example_data.set_eeg_reference(ref_channels=\"average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up band-pass filter from 0.5 - 70 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 70.00 Hz\n",
      "- Upper transition bandwidth: 17.50 Hz (-6 dB cutoff frequency: 78.75 Hz)\n",
      "- Filter length: 3381 samples (6.604 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.1s\n"
     ]
    }
   ],
   "source": [
    "data = mne.filter.filter_data(mne_example_data.get_data(), sfreq = 512, l_freq = 0.5, h_freq=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=47, n_times=248320\n",
      "    Range : 0 ... 248319 =      0.000 ...   484.998 secs\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "mne_example_data = mne.io.RawArray(data, mne_example_data.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n"
     ]
    }
   ],
   "source": [
    "recording = SingleSubjectRecording(0 << 8 + 2, mne_example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.21 GiB for an array with shape (88, 1843200) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrecording\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_latent_kmeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_gfp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_inits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Micro\\Desktop\\MainResearchProjects\\eeg_latent_space\\code\\eeg_recording.py:216\u001b[0m, in \u001b[0;36mSingleSubjectRecording.run_latent_kmeans\u001b[1;34m(self, n_states, use_gfp, n_inits)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_latent_kmeans\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_states, use_gfp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, n_inits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m    Run microstate segmentation. Gets canonical microstates using modified\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    K-Means clustering and timeseries segmentation using the dummy rule -\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m    :type n_inits: int\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    210\u001b[0m     (\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_maps,\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_segmentation,\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolarity,\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgev_tot,\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgev_gfp,\n\u001b[1;32m--> 216\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43msegment_microstates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmod_kmeans\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_gfp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_gfp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_inits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_inits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_polarity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_states \u001b[38;5;241m=\u001b[39m n_states\n",
      "File \u001b[1;32mc:\\Users\\Micro\\Desktop\\MainResearchProjects\\eeg_latent_space\\code\\microstates.py:107\u001b[0m, in \u001b[0;36msegment_microstates\u001b[1;34m(data, method, n_states, use_gfp, normalize, return_polarity, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m activation \u001b[38;5;241m=\u001b[39m maps\u001b[38;5;241m.\u001b[39mdot(data)\n\u001b[0;32m    106\u001b[0m segmentation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(np\u001b[38;5;241m.\u001b[39mabs(activation), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m map_corr \u001b[38;5;241m=\u001b[39m \u001b[43mcorr_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43msegmentation\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m gfp_corr \u001b[38;5;241m=\u001b[39m corr_vectors(data[:, peaks], maps[segmentation[peaks]]\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# compare across iterations using global explained variance (GEV) of\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# the found microstates.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Micro\\Desktop\\MainResearchProjects\\eeg_latent_space\\code\\data_utils.py:93\u001b[0m, in \u001b[0;36mcorr_vectors\u001b[1;34m(A, B, axis)\u001b[0m\n\u001b[0;32m     91\u001b[0m An \u001b[38;5;241m=\u001b[39m A \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(A, axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     92\u001b[0m Bn \u001b[38;5;241m=\u001b[39m B \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(B, axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 93\u001b[0m An \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m Bn \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(Bn, axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(An \u001b[38;5;241m*\u001b[39m Bn, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\Micro\\anaconda3\\envs\\sakuyui39\\lib\\site-packages\\numpy\\linalg\\linalg.py:2582\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m add\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mabs\u001b[39m(x), axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims)\n\u001b[0;32m   2580\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   2581\u001b[0m     \u001b[38;5;66;03m# special case for speedup\u001b[39;00m\n\u001b[1;32m-> 2582\u001b[0m     s \u001b[38;5;241m=\u001b[39m (\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m)\u001b[38;5;241m.\u001b[39mreal\n\u001b[0;32m   2583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sqrt(add\u001b[38;5;241m.\u001b[39mreduce(s, axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims))\n\u001b[0;32m   2584\u001b[0m \u001b[38;5;66;03m# None of the str-type keywords for ord ('fro', 'nuc')\u001b[39;00m\n\u001b[0;32m   2585\u001b[0m \u001b[38;5;66;03m# are valid for vectors\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.21 GiB for an array with shape (88, 1843200) and data type float64"
     ]
    }
   ],
   "source": [
    "recording.run_latent_kmeans(n_states=9, use_gfp=True, n_inits=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 47)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "microstates = recording.latent_maps\n",
    "microstates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44715825323618047"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recording.gev_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mne_example_data.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 30.78629073  32.38387228  32.53269755 ... 184.13942999 159.92214329\n",
      "  137.17126715]\n",
      " [-32.93767442 -38.67300829 -47.76458465 ... -25.41480863 -34.60075786\n",
      "  -42.55006125]\n",
      " [ -5.43664283   4.14588945  17.33972476 ... 362.70422868 329.65057379\n",
      "  298.06995285]\n",
      " ...\n",
      " [-70.90081658 -73.39596684 -80.83949145 ... -99.93996995 -96.75787581\n",
      "  -95.07881456]\n",
      " [-71.20564574 -69.23512586 -74.24628047 ... -64.02229791 -58.65491401\n",
      "  -53.85131512]\n",
      " [-36.10473468 -39.34547843 -43.1381015  ...  16.56768132  25.29825924\n",
      "   35.16601051]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.489340963862868"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_utils import corr_vectors, get_gfp_peaks\n",
    "\n",
    "activation = microstates.dot(data)\n",
    "segmentation = np.argmax(np.abs(activation), axis=0)\n",
    "print(activation)\n",
    "\n",
    "gfp_curve = None\n",
    "(gfp_peeks, gfp_curve) = get_gfp_peaks(\n",
    "    data, min_peak_dist=2, smoothing=None, smoothing_window=100\n",
    ")\n",
    "gfp_sum_sq = np.sum(gfp_curve**2)\n",
    "map_corr = corr_vectors(data, microstates[segmentation].T)\n",
    "\n",
    "gev_tot = sum((gfp_curve * map_corr) ** 2) / gfp_sum_sq\n",
    "gev_tot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import get_gfp_peaks\n",
    "class MicrostateTrainingModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_microstate, n_channels, W = None):\n",
    "        super(MicrostateTrainingModel, self).__init__()\n",
    "        weight_shape = (n_microstate, n_channels)\n",
    "        if W is None:\n",
    "            W = np.random.random(weight_shape)\n",
    "            W = torch.DoubleTensor(W)\n",
    "            norm = W.pow(2).sum(dim=1).sqrt()\n",
    "            W = W / norm.unsqueeze(-1)\n",
    "            self.W = torch.nn.Linear(n_microstate, n_channels, bias  = False)\n",
    "            self.W.weight = torch.nn.Parameter(torch.DoubleTensor(W))\n",
    "        else:\n",
    "            self.W = torch.nn.Linear(n_microstate, n_channels, bias  = False)\n",
    "            self.W.weight = torch.nn.Parameter(torch.DoubleTensor(W))\n",
    "            \n",
    "        self.loss_function = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "\n",
    "            \n",
    "    \n",
    "    def corr_vectors(self, A, B, axis=0):\n",
    "        An = A - torch.mean(A, dim=axis, keepdims=True)\n",
    "        Bn = B - torch.mean(B, dim=axis, keepdims=True)\n",
    "        new_An = An / torch.linalg.norm(An, dim=axis, keepdims=True)\n",
    "        new_Bn = Bn / torch.linalg.norm(Bn, dim=axis, keepdims=True)\n",
    "        return torch.sum(new_An * new_Bn, dim=axis)\n",
    "    \n",
    "    def forward(self, eeg_data):\n",
    "        \n",
    "        microstates_maps = self.W\n",
    "        eeg_data = torch.DoubleTensor(eeg_data)\n",
    "        activation = torch.abs(microstates_maps(eeg_data.T)).T\n",
    "        segmentation = torch.argmax(activation, axis = 0)\n",
    "        return segmentation\n",
    "        \n",
    "    def loss(self, eeg_data, segmentation, **kwargs):\n",
    "        # loss\n",
    "        (peaks, gfp_curve) = get_gfp_peaks(\n",
    "            eeg_data,\n",
    "            min_peak_dist=kwargs.pop(\"min_peak_dist\", 2),\n",
    "            smoothing=kwargs.pop(\"smoothing\", None),\n",
    "            smoothing_window=kwargs.pop(\"smoothing_window\", 100),\n",
    "        )\n",
    "        gfp_sum_sq = np.sum(gfp_curve ** 2)\n",
    "\n",
    "        eeg_data = torch.DoubleTensor(eeg_data)\n",
    "        map_corr = self.corr_vectors(eeg_data, self.W.weight[segmentation].T)\n",
    "        \n",
    "        \n",
    "        gev = torch.sum((torch.DoubleTensor(gfp_curve) * map_corr) ** 2) / gfp_sum_sq\n",
    "        \n",
    "        loss = self. loss_function(gev, torch.DoubleTensor([1]))\n",
    "        print(loss, gev)\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MicrostateTrainingModel(n_microstate=31, n_channels=47, W = microstates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mne_example_data.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation = model.forward(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3497, dtype=torch.float64, grad_fn=<MseLossBackward0>) tensor(0.4086, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.3497, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3497, dtype=torch.float64, grad_fn=<MseLossBackward0>) tensor(0.4087, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.3497, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3496, dtype=torch.float64, grad_fn=<MseLossBackward0>) tensor(0.4087, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.3496, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3496, dtype=torch.float64, grad_fn=<MseLossBackward0>) tensor(0.4087, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.3496, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3497, dtype=torch.float64, grad_fn=<MseLossBackward0>) tensor(0.4087, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.3497, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3497, dtype=torch.float64, grad_fn=<MseLossBackward0>) tensor(0.4087, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.3497, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3497, dtype=torch.float64, grad_fn=<MseLossBackward0>) tensor(0.4086, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.3497, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3498, dtype=torch.float64, grad_fn=<MseLossBackward0>) tensor(0.4086, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.3498, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3498, dtype=torch.float64, grad_fn=<MseLossBackward0>) tensor(0.4086, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.3498, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3499, dtype=torch.float64, grad_fn=<MseLossBackward0>) tensor(0.4085, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.3499, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3499, dtype=torch.float64, grad_fn=<MseLossBackward0>) tensor(0.4085, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.3499, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3500, dtype=torch.float64, grad_fn=<MseLossBackward0>) tensor(0.4084, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.3500, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3500, dtype=torch.float64, grad_fn=<MseLossBackward0>) tensor(0.4084, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.3500, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3500, dtype=torch.float64, grad_fn=<MseLossBackward0>) tensor(0.4084, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.3500, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3501, dtype=torch.float64, grad_fn=<MseLossBackward0>) tensor(0.4083, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.3501, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3501, dtype=torch.float64, grad_fn=<MseLossBackward0>) tensor(0.4083, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.3501, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3501, dtype=torch.float64, grad_fn=<MseLossBackward0>) tensor(0.4083, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.3501, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3501, dtype=torch.float64, grad_fn=<MseLossBackward0>) tensor(0.4083, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.3501, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3501, dtype=torch.float64, grad_fn=<MseLossBackward0>) tensor(0.4083, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.3501, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3501, dtype=torch.float64, grad_fn=<MseLossBackward0>) tensor(0.4083, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.3501, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3502, dtype=torch.float64, grad_fn=<MseLossBackward0>) tensor(0.4083, dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[280], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Compute the loss and its gradients\u001b[39;00m\n\u001b[0;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(data, outputs)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Adjust learning weights\u001b[39;00m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Micro\\anaconda3\\envs\\sakuyui39\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Micro\\anaconda3\\envs\\sakuyui39\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Micro\\anaconda3\\envs\\sakuyui39\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "running_loss = 0\n",
    "for i  in range(10000):\n",
    "        # Zero your gradients for every batch!\n",
    "        model.optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model.forward(data)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = model.loss(data, outputs)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        model.optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        print(loss)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "                for param in model.parameters():\n",
    "                        norm = param.pow(2).sum(dim=1).sqrt()\n",
    "                        param = param / norm.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sakuyui39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
